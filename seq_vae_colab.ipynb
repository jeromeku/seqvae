{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "\n",
    "%load_ext autoreload\n",
    "%load_ext tensorboard\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SOURCE_PATH = '/media/jeromeku/easystore/workspace/deep_learning/tensorflow/notebooks/src/'\n",
    "if not SOURCE_PATH in sys.path:\n",
    "    sys.path.append(SOURCE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from disentangled.flags import FLAGS\n",
    "FLAGS.mark_as_parsed()\n",
    "\n",
    "import disentangled.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <function create_sprites_dataset.<locals>.process_example at 0x7fcb904747b8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Cell is empty\n",
      "WARNING: Entity <function create_sprites_dataset.<locals>.process_example at 0x7fcb904747b8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Cell is empty\n",
      "WARNING:tensorflow:Entity <function create_sprites_dataset.<locals>.process_example at 0x7fcbf06e99d8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Cell is empty\n",
      "WARNING: Entity <function create_sprites_dataset.<locals>.process_example at 0x7fcbf06e99d8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Cell is empty\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=2397, shape=(), dtype=string, numpy=b'legs/pants/male/white_pants_male.png'>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data Loading\n",
    "\n",
    "sprites = sprites_datasetv2.SpritesDataset(fake_data=FLAGS.fake_data)\n",
    "\n",
    "#Sample data\n",
    "train = iter(sprites.train)\n",
    "def unmap(example):\n",
    "    seq, skin_idx,hair_idx,top_idx,pants_idx,act_idx,skin_name,hair_name, top_name,pants_name,act_name = example\n",
    "    ex_map = dict(seq=seq, \n",
    "                  skin_idx=skin_idx, \n",
    "                  hair_idx=hair_idx, \n",
    "                  top_idx=top_idx, \n",
    "                  pants_idx=pants_idx, \n",
    "                  act_idx=act_idx, \n",
    "                  skin_name=skin_name,\n",
    "                  hair_name=hair_name, \n",
    "                  top_name=top_name, \n",
    "                  pants_name=pants_name,\n",
    "                  act_name=act_name)\n",
    "    return ex_map\n",
    "\n",
    "ex = unmap(next(iter(sprites.train)))\n",
    "ex['pants_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build model\n",
    "model = models.DisentangledSequentialVAE(\n",
    "      latent_size_static=FLAGS.latent_size_static,\n",
    "      latent_size_dynamic=FLAGS.latent_size_dynamic,\n",
    "      hidden_size=FLAGS.hidden_size, channels=sprites.channels,\n",
    "      latent_posterior=FLAGS.latent_posterior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimizer and learning rate schedule\n",
    "step = tf.Variable(0, dtype=tf.int64, trainable=False) \n",
    "schedule = tf.compat.v1.train.cosine_decay(FLAGS.learning_rate, step, FLAGS.max_steps)\n",
    "optimizer = tf.keras.optimizers.Adam(schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tensorboard\n",
    "timestamp = datetime.strftime(datetime.today(), \"%y%m%d_%H%M%S\")\n",
    "logdir = 'logs/train_data'\n",
    "file_writer = tf.summary.create_file_writer(logdir + timestamp)\n",
    "file_writer.set_as_default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checkpointing\n",
    "checkpoint = tf.train.Checkpoint(step=step, optimizer=optimizer, net=model)\n",
    "ckpt_manager = tf.train.CheckpointManager(checkpoint, './tf_ckpts', max_to_keep=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sprites.train.map(lambda *x: x[0]).shuffle(1000).repeat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensors = tf.stack([t for t in dataset], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrs = tensors.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('./data/train', *arrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_objs = np.load('./data/train.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data = {\"length\": sprites.length, \"channels\": sprites.channels, \"frame_size\": sprites.frame_size}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([5, 8, 64, 64, 3])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.stack(tensors, axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrs = [t.numpy() for t in tensors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 64, 64, 3)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate(arrs, axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([8, 64, 64, 3])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.batch(FLAGS.batch_size).take(FLAGS.max_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./src/disentangled/summary.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/disentangled/summary.py\n",
    "\n",
    "def image_summary(seqs, name, step, num=None):\n",
    "  \"\"\"Visualizes sequences as TensorBoard summaries.\n",
    "\n",
    "  Args:\n",
    "    seqs: A tensor of shape [n, t, h, w, c].\n",
    "    name: String name of this summary.\n",
    "    num: Integer for the number of examples to visualize. Defaults to\n",
    "      all examples.\n",
    "  \"\"\"\n",
    "  seqs = tf.clip_by_value(seqs, 0., 1.)\n",
    "  seqs = tf.unstack(seqs[:num])\n",
    "  joined_seqs = [tf.concat(tf.unstack(seq), 1) for seq in seqs]\n",
    "  joined_seqs = tf.expand_dims(tf.concat(joined_seqs, 0), 0)\n",
    "  tf.compat.v2.summary.image(\n",
    "      name,\n",
    "      joined_seqs,\n",
    "      max_outputs=1,\n",
    "      step=step)\n",
    "\n",
    "\n",
    "def visualize_reconstruction(inputs, reconstruct, num=3, name=\"reconstruction\"):\n",
    "  \"\"\"Visualizes the reconstruction of inputs in TensorBoard.\n",
    "\n",
    "  Args:\n",
    "    inputs: A tensor of the original inputs, of shape [batch, timesteps,\n",
    "      h, w, c].\n",
    "    reconstruct: A tensor of a reconstruction of inputs, of shape\n",
    "      [batch, timesteps, h, w, c].\n",
    "    num: Integer for the number of examples to visualize.\n",
    "    name: String name of this summary.\n",
    "  \"\"\"\n",
    "  reconstruct = tf.clip_by_value(reconstruct, 0., 1.)\n",
    "  inputs_and_reconstruct = tf.concat((inputs[:num], reconstruct[:num]), axis=0)\n",
    "  image_summary(inputs_and_reconstruct, name)\n",
    "\n",
    "\n",
    "def visualize_qualitative_analysis(inputs, model, samples=1, batch_size=3,\n",
    "                                   length=8):\n",
    "  \"\"\"Visualizes a qualitative analysis of a given model.\n",
    "\n",
    "  Args:\n",
    "    inputs: A tensor of the original inputs, of shape [batch, timesteps,\n",
    "      h, w, c].\n",
    "    model: A DisentangledSequentialVAE model.\n",
    "    samples: Number of samples to draw from the latent distributions.\n",
    "    batch_size: Number of sequences to generate.\n",
    "    length: Number of timesteps to generate for each sequence.\n",
    "  \"\"\"\n",
    "  average = lambda dist: tf.reduce_mean(\n",
    "      input_tensor=dist.mean(), axis=0)  # avg over samples\n",
    "  with tf.compat.v1.name_scope(\"val_reconstruction\"):\n",
    "    reconstruct = functools.partial(model.reconstruct, inputs=inputs,\n",
    "                                    samples=samples)\n",
    "    visualize_reconstruction(inputs, average(reconstruct()))\n",
    "    visualize_reconstruction(inputs, average(reconstruct(sample_static=True)),\n",
    "                             name=\"static_prior\")\n",
    "    visualize_reconstruction(inputs, average(reconstruct(sample_dynamic=True)),\n",
    "                             name=\"dynamic_prior\")\n",
    "    visualize_reconstruction(inputs, average(reconstruct(swap_static=True)),\n",
    "                             name=\"swap_static\")\n",
    "    visualize_reconstruction(inputs, average(reconstruct(swap_dynamic=True)),\n",
    "                             name=\"swap_dynamic\")\n",
    "\n",
    "  with tf.compat.v1.name_scope(\"generation\"):\n",
    "    generate = functools.partial(model.generate, batch_size=batch_size,\n",
    "                                 length=length, samples=samples)\n",
    "    image_summary(average(generate(fix_static=True)), \"fix_static\")\n",
    "    image_summary(average(generate(fix_dynamic=True)), \"fix_dynamic\")\n",
    "\n",
    "\n",
    "def summarize_dist_params(dist, name, step, name_scope=\"dist_params\"):\n",
    "  \"\"\"Summarize the parameters of a distribution.\n",
    "\n",
    "  Args:\n",
    "    dist: A Distribution object with mean and standard deviation\n",
    "      parameters.\n",
    "    name: The name of the distribution.\n",
    "    name_scope: The name scope of this summary.\n",
    "  \"\"\"\n",
    "  with tf.compat.v1.name_scope(name_scope):\n",
    "    tf.compat.v2.summary.histogram(\n",
    "        name=\"{}/{}\".format(name, \"mean\"),\n",
    "        data=dist.mean(),\n",
    "        step=step)\n",
    "    tf.compat.v2.summary.histogram(\n",
    "        name=\"{}/{}\".format(name, \"stddev\"),\n",
    "        data=dist.stddev(),\n",
    "        step=step)\n",
    "\n",
    "\n",
    "def summarize_mean_in_nats_and_bits(inputs, units, name, step,\n",
    "                                    nats_name_scope=\"nats\",\n",
    "                                    bits_name_scope=\"bits_per_dim\"):\n",
    "  \"\"\"Summarize the mean of a tensor in nats and bits per unit.\n",
    "\n",
    "  Args:\n",
    "    inputs: A tensor of values measured in nats.\n",
    "    units: The units of the tensor with which to compute the mean bits\n",
    "      per unit.\n",
    "    name: The name of the tensor.\n",
    "    nats_name_scope: The name scope of the nats summary.\n",
    "    bits_name_scope: The name scope of the bits summary.\n",
    "  \"\"\"\n",
    "  mean = tf.reduce_mean(input_tensor=inputs)\n",
    "  with tf.compat.v1.name_scope(nats_name_scope):\n",
    "    tf.compat.v2.summary.scalar(\n",
    "        name,\n",
    "        mean,\n",
    "        step=step)\n",
    "  with tf.compat.v1.name_scope(bits_name_scope):\n",
    "    tf.compat.v2.summary.scalar(\n",
    "        name,\n",
    "        mean / units / tf.math.log(2.),\n",
    "        step=step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./src/disentangled/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/disentangled/train.py\n",
    "\n",
    "def train_step(model, optimizer, dataset, flags, summary_writer):\n",
    "    for inputs in dataset.prefetch(buffer_size=None):\n",
    "      with tf.compat.v2.summary.record_if(\n",
    "          lambda: tf.math.equal(0, global_step % FLAGS.log_steps)):\n",
    "        tf.compat.v2.summary.histogram(\n",
    "            \"image\",\n",
    "            data=inputs,\n",
    "            step=tf.compat.v1.train.get_or_create_global_step())\n",
    "\n",
    "      with tf.GradientTape() as tape:\n",
    "        features = model.compressor(inputs)  # (batch, timesteps, hidden)\n",
    "        static_sample, static_posterior = model.sample_static_posterior(\n",
    "            features, FLAGS.num_samples)  # (samples, batch, latent)\n",
    "        dynamic_sample, dynamic_posterior = model.sample_dynamic_posterior(\n",
    "            features, FLAGS.num_samples, static_sample)  # (sampl, N, T, latent)\n",
    "        likelihood = model.decoder((dynamic_sample, static_sample))\n",
    "\n",
    "        reconstruction = tf.reduce_mean(  # integrate samples\n",
    "            input_tensor=likelihood.mean()[:FLAGS.num_reconstruction_samples],\n",
    "            axis=0)\n",
    "        visualize_reconstruction(inputs, reconstruction,\n",
    "                                 name=\"train_reconstruction\")\n",
    "\n",
    "        static_prior = model.static_prior()\n",
    "        _, dynamic_prior = model.sample_dynamic_prior(\n",
    "            FLAGS.num_samples, FLAGS.batch_size, sprites_data.length)\n",
    "\n",
    "        if FLAGS.enable_debug_logging:\n",
    "          summarize_dist_params(static_prior, \"static_prior\")\n",
    "          summarize_dist_params(static_posterior, \"static_posterior\")\n",
    "          summarize_dist_params(dynamic_prior, \"dynamic_prior\")\n",
    "          summarize_dist_params(dynamic_posterior, \"dynamic_posterior\")\n",
    "          summarize_dist_params(likelihood, \"likelihood\")\n",
    "\n",
    "        static_prior_log_prob = static_prior.log_prob(static_sample)\n",
    "        static_posterior_log_prob = static_posterior.log_prob(static_sample)\n",
    "        dynamic_prior_log_prob = tf.reduce_sum(\n",
    "            input_tensor=dynamic_prior.log_prob(dynamic_sample),\n",
    "            axis=-1)  # sum time\n",
    "        dynamic_posterior_log_prob = tf.reduce_sum(\n",
    "            input_tensor=dynamic_posterior.log_prob(dynamic_sample),\n",
    "            axis=-1)  # sum time\n",
    "        likelihood_log_prob = tf.reduce_sum(\n",
    "            input_tensor=likelihood.log_prob(inputs), axis=-1)  # sum time\n",
    "\n",
    "        if FLAGS.enable_debug_logging:\n",
    "          with tf.compat.v1.name_scope(\"log_probs\"):\n",
    "            summarize_mean_in_nats_and_bits(\n",
    "                static_prior_log_prob, FLAGS.latent_size_static, \"static_prior\")\n",
    "            summarize_mean_in_nats_and_bits(\n",
    "                static_posterior_log_prob, FLAGS.latent_size_static,\n",
    "                \"static_posterior\")\n",
    "            summarize_mean_in_nats_and_bits(\n",
    "                dynamic_prior_log_prob, FLAGS.latent_size_dynamic *\n",
    "                sprites_data.length, \"dynamic_prior\")\n",
    "            summarize_mean_in_nats_and_bits(\n",
    "                dynamic_posterior_log_prob, FLAGS.latent_size_dynamic *\n",
    "                sprites_data.length, \"dynamic_posterior\")\n",
    "            summarize_mean_in_nats_and_bits(\n",
    "                likelihood_log_prob, sprites_data.frame_size ** 2 *\n",
    "                sprites_data.channels * sprites_data.length, \"likelihood\")\n",
    "\n",
    "        elbo = tf.reduce_mean(input_tensor=static_prior_log_prob -\n",
    "                              static_posterior_log_prob +\n",
    "                              dynamic_prior_log_prob -\n",
    "                              dynamic_posterior_log_prob + likelihood_log_prob)\n",
    "        loss = -elbo\n",
    "        tf.compat.v2.summary.scalar(\n",
    "            \"elbo\",\n",
    "            elbo,\n",
    "            step=tf.compat.v1.train.get_or_create_global_step())\n",
    "\n",
    "      grads = tape.gradient(loss, model.variables)\n",
    "      grads, global_norm = tf.clip_by_global_norm(grads, FLAGS.clip_norm)\n",
    "      grads_and_vars = list(zip(grads, model.variables))  # allow reuse in py3\n",
    "      if FLAGS.enable_debug_logging:\n",
    "        with tf.compat.v1.name_scope(\"grads\"):\n",
    "          tf.compat.v2.summary.scalar(\n",
    "              \"global_norm_grads\",\n",
    "              global_norm,\n",
    "              step=tf.compat.v1.train.get_or_create_global_step())\n",
    "          tf.compat.v2.summary.scalar(\n",
    "              \"global_norm_grads_clipped\",\n",
    "              tf.linalg.global_norm(grads),\n",
    "              step=tf.compat.v1.train.get_or_create_global_step())\n",
    "        for grad, var in grads_and_vars:\n",
    "          with tf.compat.v1.name_scope(\"grads\"):\n",
    "            tf.compat.v2.summary.histogram(\n",
    "                \"{}/grad\".format(var.name),\n",
    "                data=grad,\n",
    "                step=tf.compat.v1.train.get_or_create_global_step())\n",
    "          with tf.compat.v1.name_scope(\"vars\"):\n",
    "            tf.compat.v2.summary.histogram(\n",
    "                var.name,\n",
    "                data=var,\n",
    "                step=tf.compat.v1.train.get_or_create_global_step())\n",
    "      optimizer.apply_gradients(grads_and_vars, global_step)\n",
    "\n",
    "    is_log_step = global_step.numpy() % FLAGS.log_steps == 0\n",
    "    is_final_step = global_step.numpy() == FLAGS.max_steps\n",
    "    if is_log_step or is_final_step:\n",
    "      checkpoint_manager.save()\n",
    "      print(\"ELBO ({}/{}): {}\".format(global_step.numpy(), FLAGS.max_steps,\n",
    "                                      elbo.numpy()))\n",
    "      with tf.compat.v2.summary.record_if(True):\n",
    "        val_data = sprites_data.test.take(20)\n",
    "        inputs = next(iter(val_data.shuffle(20).batch(3)))[0]\n",
    "        visualize_qualitative_analysis(inputs, model,\n",
    "                                       FLAGS.num_reconstruction_samples)\n",
    "\n",
    "    writer.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
